---
title: "Scrape Spiegel Online Articles"
author: "Jan Dix <jan.dix@uni-konstanz.de>"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Dependencies

```{r, eval=FALSE}
library(newsanchor) # download newspaper articles
library(rvest)      # web scraping tools
library(magrittr)   # pipes
library(stringr)    # string/character manipulations
```



## Download Spiegel Online articles and their corresponding URLs

# download meta data

```{r, eval=FALSE}
# get heaadlines published by spiegel online
response <- get_headlines(source = "spiegel-online") 
```

## Define a function to scrape article body

```{r, eval=FALSE}
get_article_body <- function (url) {
  # download and parse website
  html <- read_html(url)                    
  
  # parse content
  html %>% 
    html_nodes("div.article-section p") %>% # extract all paragraphs within class 'article-section'
    html_text() %>%                         # extract content of the p tags
    str_replace_all("\n", "") %>%           # replace all line breaks
    paste(collapse = " ")                   # join all paragraphs into one string
}

```


## Apply the new function

```{r, eval=FALSE}
# apply function to the given URLs
response$results_df$body <- sapply(response$results_df$url, get_article_body)
```



## What else could be done?

- check if article has multiple pages
- fetch comments
- download all images




